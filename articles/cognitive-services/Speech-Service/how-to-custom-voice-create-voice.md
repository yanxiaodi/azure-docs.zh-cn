---
title: 创建自定义语音语音服务
titleSuffix: Azure Cognitive Services
description: 如果已准备好上传数据, 请切换到自定义语音门户。 创建或选择一个自定义语音项目。 项目必须共享正确的语言/区域设置和性别属性作为要用于语音训练的数据。
services: cognitive-services
author: erhopf
manager: nitinme
ms.service: cognitive-services
ms.subservice: speech-service
ms.topic: conceptual
ms.date: 07/05/2019
ms.author: erhopf
ms.openlocfilehash: 0fdc58ba54c63ba7dd6b74f56aa91e9c2b3c0936
ms.sourcegitcommit: 7c4de3e22b8e9d71c579f31cbfcea9f22d43721a
ms.translationtype: MT
ms.contentlocale: zh-CN
ms.lasthandoff: 07/26/2019
ms.locfileid: "68562834"
---
# <a name="create-a-custom-voice"></a>创建自定义语音

在[准备自定义语音数据](how-to-custom-voice-prepare-data.md)时, 我们介绍了可用于训练自定义语音和不同格式要求的不同数据类型。 准备好数据后, 可以开始将其上传到[自定义语音门户](https://aka.ms/custom-voice-portal), 或通过自定义语音训练 API。 本文介绍通过门户训练自定义语音的步骤。

> [!NOTE]
> 本页假设你已阅读[自定义语音入门](how-to-custom-voice.md)并[为自定义语音准备数据](how-to-custom-voice-prepare-data.md), 并已创建自定义语音项目。

检查自定义语音: 语言所支持的语言[以进行自定义](language-support.md#customization)。

## <a name="upload-your-datasets"></a>上传数据集

如果已准备好上传数据, 请切换到[自定义语音门户](https://aka.ms/custom-voice-portal)。 创建或选择一个自定义语音项目。 项目必须共享正确的语言/区域设置和性别属性作为要用于语音训练的数据。 例如, 如果您`en-GB`拥有的音频录音使用的是英国英语, 则选择。

中转到 "**数据**" 选项卡, 然后单击 "**上传数据**"。 在向导中, 选择与准备就绪的正确数据类型。

你上载的每个数据集都必须满足你选择的数据类型的要求。 在上载数据之前, 请务必正确设置数据的格式。 这可确保自定义语音服务将准确处理数据。 请参阅为[自定义语音准备数据](how-to-custom-voice-prepare-data.md), 并确保数据已本来格式。

> [!NOTE]
> 免费订阅 (F0) 用户可同时上传两个数据集。 标准订阅 (S0) 用户可以同时上传五个数据集。 如果达到限制，请先等待，直至至少其中一个数据集导入完毕。 然后重试。

> [!NOTE]
> 每个订阅允许导入的数据集的最大数目为10个 .zip 文件 (对于免费订阅 (F0) 用户) 和 500 (对于标准订阅 (S0) 用户)。

命中 "上传" 按钮后, 将自动验证数据集。 数据验证包括对音频文件的一系列检查, 以验证其文件格式、大小和采样速率。 如果有错误, 请修复错误, 然后重新提交。 成功启动数据导入请求时, 应会在数据表中看到对应于刚上传的数据集的条目。

下表显示了已导入数据集的处理状态：

| 状态 | 含义 |
| ----- | ------- |
| 正在处理 | 数据集已接收并正在处理。 |
| 已成功 | 您的数据集已经过验证, 现可用于构建语音模型。 |
| 已失败 | 数据集在处理过程中因多种原因而失败, 例如, 文件错误、数据问题或网络问题。 |

验证完成后, 可以在**最谈话**列中查看每个数据集的匹配最谈话总数。 如果所选的数据类型需要长时间音频分段, 此列只会根据您的脚本或通过语音脚本服务为您划分的最谈话反映。 可以进一步下载验证的数据集, 以查看成功导入的最谈话的详细结果及其映射脚本。 提示: 长时间音频分段可能需要超过一小时才能完成数据处理。

对于 en-us 和 zh-chs 数据集, 可以进一步下载报表, 查看每个录制的发音评分和噪音级别。 发音评分范围为 0 到 100。 评分低于 70 通常表示语音错误或脚本不匹配。 口音重可能会降低发音分数，影响生成的数字语音。

信噪比 (SNR) 高表明音频中的噪音少。 通过专业录音棚录音通常可以达到 50+ 的 SNR。 音频的 SNR 低于 20 可能导致生成的语音中出现明显的噪音。

考虑重写录制发音分数低或信噪比不佳的表述。 如果无法重新录制，可以从数据集中排除这些话语。

## <a name="build-your-custom-voice-model"></a>构建自定义语音模型

验证数据集后, 可以使用它来生成自定义语音模型。

1.  导航到**文本到语音 > 自定义语音 > 培训**。

2.  单击 "**训练模型**"。

3.  接下来, 输入**名称**和**描述**以帮助您识别此模型。

    请谨慎选择名称。 此处输入的名称将是在 SSML 输入过程中在请求中指定语音合成所需语音时使用的名称。 只允许使用字母、数字和几个标点符号, 如-、 \_和 (', ')。 为不同的语音模型使用不同的名称。

    通常使用“说明”字段来记录创建模型时所使用的数据集的名称。

4.  从 "**选择定型数据**" 页中, 选择一个或多个要用于定型的数据集。 提交最谈话之前, 请检查其数量。 可以从任意数量的最谈话开始, 为 en-us 和 zh-chs 语音模型。 对于其他区域设置, 你必须选择超过2000最谈话才能训练语音。

    > [!NOTE]
    > 将从训练中删除重复的音频名称。 请确保所选数据集不包含跨多个 .zip 文件的相同音频名称。

    > [!TIP]
    > 对于质量结果, 需要使用同一发言人中的数据集。 当你为定型提交的数据集包含的总数小于6000个不同的最谈话时, 你将通过统计参数合成技术来训练语音模型。 如果定型数据超过6000个不同最谈话的总数, 则会开始使用串联合成技术进行训练。 通常, 连接技术会导致更自然、更高保真的语音结果。 如果你想要使用最新的神经 TTS 技术来训练模型, 该技术可产生等效于公开提供的[神经声音](language-support.md#neural-voices)的数字语音,[请联系自定义语音团队](mailto:speechsupport@microsoft.com)。

5.  单击 "**训练**" 开始创建语音模型。

定型表显示与这个新创建的模型相对应的新条目。 该表还显示状态:处理、成功、失败。

显示的状态反映将数据集转换为语音模型的过程, 如下所示。

| 状态 | 含义 |
| ----- | ------- |
| 正在处理 | 正在创建您的语音模型。 |
| 已成功 | 你的语音模型已创建并可以部署。 |
| 已失败 | 由于多种原因 (例如不可见的数据问题或网络问题), 您的语音模型在定型中已失败。 |

训练时间因处理的音频数据量而异。 通常情况下，处理数百个表述需要大约 30 分钟的时间，处理 20,000 个表述需要 40 小时。 模型定型成功后, 可以开始对其进行测试。

> [!NOTE]
> 免费订阅 (F0) 用户可以同时训练一种语音字体。 标准订阅 (S0) 用户可以同时训练三个声音。 如果达到限制，请先等待，直至至少其中一种语音字体训练完毕，然后再试。

> [!NOTE]
> 每个订阅允许的最大语音模型数量是为免费订阅 (F0) 用户使用10个模型, 为标准订阅 (S0) 用户提供100。

## <a name="test-your-voice-model"></a>测试语音模型

成功生成语音字体以后，可以对其先测试后部署，然后就可以使用了。

1.  导航到**文本到语音 > 自定义语音 > 测试**。

2.  单击 "**添加测试**"。

3.  选择要测试的一个或多个模型。

4.  提供要语音朗读的文本。 如果您已选择一次测试多个模型, 则相同的文本将用于测试不同的模型。

    > [!NOTE]
    > 文本的语言必须与语音字体的语言相同。 只能测试已成功训练的模型。 此步骤只支持纯文本。

5.  单击“创建”。

提交测试请求后, 将返回到 "测试" 页。 表中现在会有新请求的对应条目以及状态列。 可能需要数分钟来合成语音。 当 "状态" 列显示 "已**成功**" 时, 可以播放音频, 或下载文本输入 (.txt 文件) 和音频输出 (.wav 文件), 并进一步 audition 其质量。

您还可以在选择用于测试的每个模型的详细信息页中找到测试结果。 进入 "**培训**" 选项卡, 然后单击模型名称以输入模型详细信息页。

## <a name="create-and-use-a-custom-voice-endpoint"></a>创建和使用自定义语音端点

成功创建并测试语音模型以后，即可在自定义的文本转语音终结点中部署它。 然后即可在通过 REST API 发出文本转语音请求时使用此终结点来替代常用的终结点。 自定义终结点只能由用于部署该字体的订阅调用。

若要创建新的自定义语音终结点, 请转到**文本到语音 > 自定义语音 > 部署**。 选择 "**添加终结点**", 然后输入自定义终结点的**名称**和**描述**。 然后选择想要与此终结点关联的自定义语音模型。

单击 "**添加**" 按钮后, 会在终结点表中看到新终结点的条目。 新终结点的实例化可能需要数分钟。 当部署状态为“成功”时，终结点便可供使用。

> [!NOTE]
> 免费订阅 (F0) 用户只能部署一个模型。 标准订阅 (S0) 用户最多可以创建50个终结点, 每个终结点都有自己的自定义语音。

> [!NOTE]
> 若要使用自定义语音, 必须指定语音模型名称, 直接在 HTTP 请求中使用自定义 URI, 并使用同一订阅来通过 TTS 服务的身份验证。

部署终结点后, 终结点名称将显示为链接。 单击此链接可显示特定于你的终结点的信息, 例如终结点密钥、终结点 URL 和示例代码。

也可通过自定义语音门户对终结点进行联机测试。 若要测试终结点, 请从**终结点详细信息**页中选择 "**检查终结点**"。 此时会显示终结点测试页。 在文本框中输入要口述的文本 (以纯文本[格式或 SSML 格式](speech-synthesis-markup.md))。 若要收听以自定义语音字体朗读的文本，请选择“播放”。 此测试功能将针对自定义语音合成使用情况收费。

从功能上说，自定义终结点与用于文本转语音请求的标准终结点相同。 有关详细信息，请参阅 [REST API](rest-text-to-speech.md)。

## <a name="next-steps"></a>后续步骤

* [向导记录语音示例](record-custom-voice-samples.md)
* [文本到语音 API 参考](rest-text-to-speech.md)
